import json
import milvus_db_manage

#geo_corpus_file_path = '/Users/liunian/Downloads/personal/之江实验室/DDE/大模型/测评体系/原始语料/geo-oa-paper-2000.jsonl'
geo_wiki_corpus_file_path = '/Users/liunian/Downloads/personal/之江实验室/DDE/大模型/测评体系/原始语料/v0.9-20250616-43B-2000.jsonl'
geo_oa_paper_corpus_file_path = '/Users/liunian/Downloads/personal/之江实验室/DDE/大模型/测评体系/原始语料/geo-oa-paper-2000.jsonl'
chunk_size = 1000


def read_dataset(file_path):
    dataset = []
    with open(file_path, 'r') as f:
        for line in f:
            line_s = line.strip()
            item = json.loads(line_s)
            text = item['text']
            dataset.append(text)
    return dataset


def start_emb_text_into_milvus(input_file, limit_size=10):
    corpus_list = read_dataset(input_file)
    corpus_sub_list = corpus_list[0:limit_size]

    print(f'corpus count:{len(corpus_sub_list)}')
    milvus_db_manage.insert_text_to_milvus(corpus_sub_list)


def read_jsonl_in_chunks(file_path, chunk_size=1000):
    """
    逐块读取大型JSONL文件的生成器函数

    参数:
        file_path: JSONL文件的路径
        chunk_size: 每个块包含的行数（记录数）

    返回:
        生成器，每次 yield 一个包含 chunk_size 条记录的列表
    """
    current_chunk = []
    with open(file_path, 'r', encoding='utf-8') as f:
        for line_num, line in enumerate(f, 1):
            # 跳过空行
            if not line.strip():
                continue
            try:
                data = json.loads(line)
                current_chunk.append(data)
            except json.JSONDecodeError as e:
                print(f"Warning: 在第 {line_num} 行解析JSON出错: {e}")
                # 可以选择跳过错误行或记录错误
                continue

            # 当块达到预定大小时，yield 并重置块
            if len(current_chunk) >= chunk_size:
                yield current_chunk
                current_chunk = [] # 重置当前块

        # 文件读取结束后，yield 最后不足 chunk_size 的剩余数据
        if current_chunk:
            yield current_chunk





def start_insert_huge_file_into_milvus(input_file):
    corpus_list = []
    for chunk_id, chunk in enumerate(read_jsonl_in_chunks(input_file, chunk_size)):
        print(f"正在处理第 {chunk_id} 个块，包含 {len(chunk)} 条记录...")

        for line_json in chunk:
            text = line_json['text']
            corpus_list.append(text)

        milvus_db_manage.insert_text_to_milvus(corpus_list)
        corpus_list = []


def search_text_list():
    abstract_list = [
        """Subducting slabs transport carbon to deep mantle depths and release it into the overlying mantle wedge and lithospheric mantle through multiple mechanisms, including mechanical removal via diapirism, metamorphic decarbonization, carbonate dissolution and parting melting. Identifying the dominant carbon recycling mechanism responsible for carbonation of subcontinental lithospheric mantle (SCLM) remains challenging, yet it is critical for understanding the genesis of post-collisional carbonatites and associated rare earth element deposits. To address this issue, we investigate the Li isotopic systematics of typical post-collisional carbonatite-alkalic complexes from Mianning-Dechang (MD), Southeast Tibet. Our results show that the less-evolved magmas (lamprophyres) have mantle-like or slightly lower $\delta^{7}\mathrm{Li}$ values $(0.3\%o-3.6\%o)$ with limited variability, contrasting sharply with the wider $\delta^{7}\mathrm{Li}$ range observed in associated carbonatites and syenites. We interpret this dichotomy as reflecting distinct processes: while the variable and anomalous $\delta^{7}\mathrm{Li}$ values in differentiated rocks (carbonatites and syenites) were caused by late-stage magmatic-hydrothermal processes (including biotite fractionation, fluid exsolution and hydrothermal alteration), the lamprophyres retain the primary Li isotopic signature of their mantle source. Together with their arc-like trace element and EM1-EM2-type Sr-Nd-Pb isotopic signatures, such mantle-like or slightly lower $\delta^{7}\mathrm{Li}$ values of the lamprophyres preclude carbon derivation from high- $\cdot\delta^{7}\mathrm{Li}$ reservoirs (altered oceanic crust, serpentinites) and recycling of sedimentary carbon through metamorphic decarbonization or dissolution. Instead, these features indicate that the carbon was predominantly transported into the mantle source via partial melting of subducted carbonate-bearing sediments. This study demonstrates that Li isotopes can serve as a tracer for identifying the mechanism of carbon recycling in collision zones.  """,
        """Climate change is significantly influenced by both clouds and Earth’s surface temperature (EST). While numerous studies have investigated clouds and EST separately, the extent of clouds’ impact on EST remains unclear. Based on the inspiration and limitation of cloud radiative effect (CRE), this study provides a pioneering attempt to propose a novel indicator, cloud radiative effect on surface temperature (CREST), aiming to quantify how clouds affect EST globally while also analyzing the physical mechanism. Using reanalysis and remotely sensed data, a phased machine learning scheme in combination of surface energy balance theory is proposed to estimate EST under all-sky and hypothetical clear-sky conditions in stages, thereby estimating the newly defined CREST by subtracting the hypothetical clear-sky EST from the all-sky EST. The inter-annual experiments reveal the significant spatial heterogeneity in CREST across land, ocean, and ice/snow regions. As a global offset of the heterogeneity, clouds exhibit a net warming effect on global surface temperature on an annual scale (e.g., $0.26\mathrm{K}$ in 1981), despite their ability to block sunlight. However, the net warming effect has gradually weakened to nearly zero over the past four decades (e.g., only $0.06\mathrm{~K~}$ in 2021), and it’s even possible to transform into a cooling effect, which might be good news for mitigating the global warming.  """,
        "In isolated adult rat myocytes, we tested the hypothesis that metabolic inhibition and simulated ischemia regulate the NADH/NAD+ redox couple with concomitant impairment of energy-dependent process, including contraction and maintenance of high-energy phosphate stores. We developed a method to examine the relationship among the redox couple, ATP content, and contractile performance in single cells under several conditions analogous to myocardial ischemia, with and without reperfusion. Myocytes were paced at 1 Hz while cell contraction and NADH fluorescence were determined simultaneously for single cells at 37 degrees C. Cells were exposed to cyanide and 2-deoxy-D-glucose (metabolic inhibition) or to metabolic inhibition plus 12 mM KCl and 20 mM lactate at pH 6.5 (simulated ischemia). Pyridine nucleotide fluorescence signals from single cells studied in this fashion could be modulated by metabolic inhibitors in a manner similar to that classically described for isolated mitochondria. Metabolic inhibition or simulated ischemia quickly produced maximal reduction of NAD+ to NADH. When cells were exposed to simulated ischemia for 10 min, then superfused with glucose-containing control buffer, 28% of cells exposed to conditions of simulated ischemia developed hypercontracture on reperfusion. Hypercontracture developed despite mitochondrial electron transport being reestablished. When myocyte suspensions in a cuvette were studied spectrofluorimetrically, the pyridine nucleotide fluorescence response to metabolic inhibitors was similar to that for a single cell. This permitted correlation of ATP determinations on cells in suspension with contractile and fluorescence measurements from single myocytes. In the absence of glycolysis there is correspondence among loss of electron transport, decline in high-energy phosphate concentration, and decline in contraction. Irreversible disruption of the electron transport process does not appear to be an early event in ischemic injury.",
        "Intends to teach students the following: to apply basic physical principles to realistic situations; to solve realistic problems; to resolve contradictions between their preconceptions and the laws of physics; and, to organize the ideas of physics into an integrated hierarchy.",
        "Fox Saturday Baseball is an American television presentation of Major League Baseball (MLB) games produced by Fox Sports for the Fox network on Saturday afternoons.  Fox's coverage includes 4 weeks worth of coverage as of 2023. Coverage usually includes 2 to 4 separate games all starting at 4PM ET, local affiliates air the game of most interest to their audience.  History Fox has used numerous scheduling formulas for its Saturday regular season coverage. These have often changed based on the rights granted by new television contracts, and the pregame programs that the network has chosen to air.  From 1996 to 2006, Fox began its weekly game telecasts on the Saturday of Memorial Day weekend or the weekend before. The selection of games varied on a regional basis, and the start times were staggered based on region. A half-hour pregame show aired at 12:30 p.m. Eastern Time, followed by game broadcasts held at 1 p.m. in the Eastern and Central Time Zones. West Coast games did not air until 4 p.m. Eastern Time (1 p.m. in the Pacific Time Zone). All of these games were exclusive to the broadcast network, and as a result, Fox's exclusivity window lasted through the entire afternoon.  In 2007, Fox began airing games every Saturday during the season. A new scheduling format was devised, in which all of the regional games started simultaneously. Fox moved the pregame, which became part of the exclusive game window, to 3:30 p.m. Eastern Time. All of the Fox games would then start at 3:55 p.m. Eastern Time, regardless of region. This format gave more leeway for teams not being shown on Fox to schedule daytime games. Fox's exclusivity began at the start of the pregame at 3:30 and ran until 7 p.m. Eastern.  Fox discontinued its pregame show in 2009, with the telecasts now beginning at 4 p.m. Eastern and the game time being pushed to 4:10. Fox gave up the first half-hour of its exclusivity, with its window now beginning at 4 p.m. Eastern Time. This scheduling formula was used through 2011 for the regular season. Beginning in 2010, several of the Saturday games aired in prime time during the spring. These telecasts used an exclusivity window from 7 to 10:30 p.m. Eastern Time, as the network revived a pregame show for these games, airing at 7 p.m. with the game at 7:15.  In 2012, the pregame show returned full-time, prompting another change in scheduling. The normal scheduling in 2012 and 2013 was for the pregame airing at either 12:30 or 3:30 p.m. Eastern Time. The pregame is not a part of Fox's exclusive window, which began with the game telecast starting a half-hour later. The scheduling did not change for the spring prime time games, however, as the scheduling for these games remained the same as in 2010 and 2011. However these games began being branded as Baseball Night in America games instead of Fox Saturday Baseball.  In 2021 and 2022, Fox did not air Fox Saturday Baseball afternoon games, instead moving its entire MLB schedule to primetime Baseball Night",
        """X.28 is an ITU-T standard specifying the interface between asynchronous character-mode data terminal equipment (DTE), such as computer terminals, and a Packet Assembler/Disassembler (PAD) that connects the DTE to a packet switched network such as an X.25 network.  External links X.28 standard at ITU site  ITU-T recommendations ITU-T X Series Recommendations __label__0, In HTTP networking, typically on the World Wide Web, referer spoofing (based on a canonised misspelling of "referrer") sends incorrect referer information in an HTTP request in order to prevent a website from obtaining accurate data on the identity of the web page previously visited by the user.  Overview Referer spoofing is typically done for data privacy reasons, in testing, or in order to request information (without genuine authority) which some web servers may only supply in response to requests with specific HTTP referers.  To improve their privacy, individual browser users may replace accurate referer data with inaccurate data, though many simply suppress their browser's sending of any referer data. Sending no referrer information is not technically spoofing, though sometimes also described as such.  In software, systems and networks testing, and sometimes penetration testing, referer spoofing is often just part of a larger procedure of transmitting both accurate and inaccurate as well as  expected and unexpected input to the HTTPD system being tested and observing the results.  While many websites are configured to gather referer information and serve different content depending on the referer information obtained, exclusively relying on HTTP referer information for authentication and authorization purposes is not a genuine computer security measure.  HTTP referer information is freely alterable and interceptable, and is not a password, though some poorly configured systems treat it as such.  Application Some websites, especially many image hosting sites, use referer information to secure their materials: only browsers arriving from their web pages are served images. Additionally a site may want users to click through pages with advertisements before directly being able to access a downloadable file – using the referring page or referring site information can help a site redirect unauthorized users to the landing page the site would like to use.  If attackers acquire knowledge of these approved referrers, which is often trivial because many sites follow a common template, they can use that information combined with this to exploit and gain access to the materials.  Spoofing often allows access to a site's content where the site's web server is configured to block browsers that do not send referer headers. Website owners may do this to disallow hotlinking.  It can also be used to defeat referer checking controls that are used to mitigate Cross-Site Request Forgery attacks.  Tools Several software tools exist to facilitate referer spoofing in web browsers. Some are extensions to popular browsers such as Mozilla Firefox or Internet Explorer, which may provide facilities to customise and manage referrer URLs for each website the user visits.  Other tools include proxy servers, to which an individual configures their browser to send all HTTP requests. The proxy then forwards different headers to the intended website, usually removing or modify""",
        """X.3 is an ITU-T standard indicating what functions are to be performed by a Packet Assembler/Disassembler (PAD) when connecting character-mode data terminal equipment (DTE), such as a computer terminal, to a packet switched network such as an X.25 network, and specifying the parameters that control this operation.  The following is list of X.3 parameters associated with a PAD: 1 PAD recall using a character 2 Echo 3 Selection of data forwarding character 4 Selection of idle timer delay 5 Ancillary device control 6 Control of PAD service signals 7 Operation on receipt of break signal 8 Discard output 9 Padding after carriage return 10 Line folding 11 DTE speed 12 Flow control of the PAD 13 Linefeed insertion after carriage return 14 Padding after linefeed 15 Editing 16 Character delete 17 Line delete 18 Line display 19 Editing PAD service signals 20 Echo mask 21 Parity treatment 22 Page wait  References  External links X.3 standard at ITU site Cisco Web Page Definition of X.3 parameters  Networking standards X.25 __label__0, Noweb, stylised in lowercase as noweb, is a literate programming tool, created in 1989–1999 by Norman Ramsey, and designed to be simple, easily extensible and language independent.  As in WEB and CWEB, the main components of Noweb are two programs: "notangle", which extracts 'machine' source code from the source texts, and "noweave", which produces nicely-formatted printable documentation.  Noweb supports TeX, LaTeX, HTML, and troff back ends and works with any programming language. Besides simplicity this is the main advantage over WEB, which needs different versions to support programming languages other than Pascal. (Thus the necessity of CWEB, which supports C and similar languages.)  Noweb's input  A Noweb input text contains program source code interleaved with documentation. It consists of so-called chunks that are either documentation chunks or code chunks.  A documentation chunk begins with a line that starts with an at sign (@) followed by a space or newline. A documentation chunk has no name. Documentation chunks normally contain LaTeX, but Noweb is also used with HTML, plain TeX, and troff.  Code chunks are named. A code chunk begins with  <<chunk name>>=  on a line by itself. The double left angle bracket (<<) must be in the first column.  Each chunk is terminated by the beginning of another chunk. If the first line in the file does not mark the beginning of a chunk, it is assumed to be the first line of a documentation chunk.  Code chunks aren't treated specially by Noweb's tools—they may be placed in any order and, when needed, they are just concatenated. Chunk references in code are dereferenced and the whole requested source code is extracted.  Example of a simple Noweb program  This is an example of a "hello world" program with documentation: \section{Hello world}  Today I awoke and decided to write some code, so I started to write Hello World in \textsf C.  <<hello.c>>= /* <<license>> */ #include <stdio.h>  int main(int argc, char *argv[]) { printf("Hello World!\n"); return 0; } @ \noindent \ldots then I did the same in PHP.  <<hello.php>>= <?php /* <<license>> */ echo "Hello world!\n"; ?> @ \section{License} Later the same day some lawyer reminded me about licenses. So, here it is:  <<license>>= This work is placed in the public domain. Assuming that the above code is placed in a file named 'hello.nw', the command to extract the human-readable document in HTML format is:  noweave -filter l2h -index -html hello.nw | htmltoc > hello.html  ... and in LaTeX format:  noweave -index -latex hello.nw > hello.tex  To extract machine source code:  notangle -Rhello.c hello.nw > hello.c  notangle -Rhello.php hello.nw > hello.php  Compatibility Noweb defines a specific file format and a file is likely to interleave three different formats (Noweb, LaTeX and the language used for the software). This is not recognised by other software development tools and consequently using Noweb excludes the use of UML or code documentation""",
        """ACM Transactions on Graphics (TOG) is a bimonthly peer-reviewed scientific journal that covers the field of computer graphics.  The editor-in-chief is Carol O'Sullivan (Trinity College Dublin). According to the Journal Citation Reports, the journal had a 2020 impact factor of 5.414. The journal ranks 1st in computer graphics publications, according to Google Scholar Metrics.  History It was established in 1982 and is published by the Association for Computing Machinery. TOG publishes two special issues for ACM SIGGRAPH's conference proceedings. Starting in 2003, all papers accepted for presentation at the annual SIGGRAPH conference are printed in a special summer issue of the journal. Beginning in 2008, papers presented at SIGGRAPH Asia are printed in a special November/December issue.  References  External links  Computer graphics Computer science journals Transactions on Graphics Bimonthly journals __label__0, Cadena Salsoul is an entertainment-focused salsa radio network in Puerto Rico.  The SalSoul Network, made up of two simulcast FM facilities, has been top rated in every significant demographic since 1986. In the important age groups, the network often doubled the audience of the number two station in this market of 125 stations and 3.5 million persons.  The WPRM-FM and WIVA-FM network made up the first instance in the U.S. of using two FM signals to cover all of a large market. WPRM covers San Juan and Ponce; WIVA covers Mayagüez and Arecibo. They also added WRIO Ponce to the network. Together, they cover the "consolidated" market favored by advertising agencies. In a survey conducted in 2015, listeners ranked them the third best station, after KQ 105 FM and Z-93 FM. Through use of parallel clustering, spots could be sold locally on either signal for smaller retail accounts.  Since December 26, 2012, and after 53 years broadcasting on the frequency of 98.5 FM, the radio station, with the approval of the Federal Communications Commission in the United States, has changed to 99.1 FM for the best coverage.  Programming La Perrera Móntala con Salsoul El Show de Jesse y Bebe El Bollete SalSoul en la Noche  References  External links  Propulsor de una radio revolucionaria (Spanish)     Puerto Rican radio Radio stations established in 1959 1959 establishments in Puerto Rico Salsa music""",
        """In computer science, recursion is a method of solving a computational problem where the solution depends on solutions to smaller instances of the same problem. Recursion solves such recursive problems by using functions that call themselves from within their own code. The approach can be applied to many types of problems, and recursion is one of the central ideas of computer science.  Most computer programming languages support recursion by allowing a function to call itself from within its own code. Some functional programming languages (for instance, Clojure) do not define any looping constructs but rely solely on recursion to repeatedly call code. It is proved in computability theory that these recursive-only languages are Turing complete; this means that they are as powerful (they can be used to solve the same problems) as imperative languages based on control structures such as  and .  Repeatedly calling a function from within itself may cause the call stack to have a size equal to the sum of the input sizes of all involved calls. It follows that, for problems that can be solved easily by iteration, recursion is generally less efficient, and, for large problems, it is fundamental to use optimization techniques such as tail call optimization.  Recursive functions and algorithms A common algorithm design tactic is to divide a problem into sub-problems of the same type as the original, solve those sub-problems, and combine the results. This is often referred to as the divide-and-conquer method; when combined with a lookup table that stores the results of previously solved sub-problems (to avoid solving them repeatedly and incurring extra computation time), it can be referred to as dynamic programming or memoization.  Base case  A recursive function definition has one or more base cases, meaning input(s) for which the function produces a result trivially (without recurring), and one or more recursive cases, meaning input(s) for which the program recurs (calls itself).  For example, the factorial function can be defined recursively by the equations  and, for all , . Neither equation by itself constitutes a complete definition; the first is the base case, and the second is the recursive case. Because the base case breaks the chain of recursion, it is sometimes also called the "terminating case".  The job of the recursive cases can be seen as breaking down complex inputs into simpler ones.  In a properly designed recursive function, with each recursive call, the input problem must be simplified in such a way that eventually the base case must be reached.  (Functions that are not intended to terminate under normal circumstances—for example, some system and server processes—are an exception to this.)  Neglecting to write a base case, or testing for it incorrectly, can cause an infinite loop.  For some functions (such as one that computes the series for ) there is not an obvious base case implied by the input data; for these one may add a parameter (s""",
        """A programming model is an execution model coupled to an API or a particular pattern of code.  In this style, there are actually two execution models in play: the execution model of the base programming language and the execution model of the programming model.  An example is Spark where  Java is the base language, and Spark is the programming model.  Execution may be based on what appear to be library calls. Other examples include the POSIX Threads library and Hadoop's MapReduce. In both cases, the execution model of the programming model is different from that of the base language in which the code is written. For example, the C programming language has no behavior in its execution model for input/output or thread behavior. But such behavior can be invoked from C syntax, by making what appears to be a call to a normal C library.  What distinguishes a programming model from a normal library is that the behavior of the call cannot be understood in terms of the language the program is written in. For example, the behavior of calls to the POSIX thread library cannot be understood in terms of the C language. The reason is that the call invokes an execution model that is different from the execution model of the language. This invocation of an outside execution model is the defining characteristic of a programming model, in contrast to a programming language.  In parallel computing, the execution model often must expose features of the hardware in order to achieve high performance. The large amount of variation in parallel hardware causes a concurrent need for a similarly large number of parallel execution models. It is impractical to make a new language for each execution model, hence it is a common practice to invoke the behaviors of the parallel execution model via an API. So, most of the programming effort is done via parallel programming models rather than parallel languages. The terminology around such programming models tends to focus on the details of the hardware that inspired the execution model, and in that insular world the mistaken belief is formed that a programming model is only for the case when an execution model is closely matched to hardware features.  References  Computer programming""",
        """![Image 1: marine navigation design image]() Introduction ------------ The maritime industry, with its vast expanse and inherent challenges, necessitates a robust framework of safety and preparedness. At the heart of this framework lies the indispensable need for comprehensive training in navigation, seamanship, and workplace safety. These areas are critical in ensuring that maritime operations are conducted safely, efficiently, and in compliance with international standards and regulations. Navigation, the art and science of determining a vessel's path and position, is fundamental to safe maritime operations. Seamanship, encompassing the skills required to operate a vessel safely, is equally vital. Workplace safety ensures a secure environment for all personnel on board, minimizing the risk of accidents and injuries. Together, these three pillars form the bedrock of maritime safety, underscoring the value of training and the development of proficiency among maritime personnel. This article aims to delve into the significance of training in these crucial areas, highlighting how it directly influences the safety and success of maritime operations. By emphasizing the importance of equipping personnel with the necessary skills and knowledge, we underscore the role of training in fostering a culture of safety, preparedness, and excellence in the maritime industry. Table of Contents * [The Value of Training in Maritime Safety: Emphasizing the importance of training personnel in navigation, seamanship, and workplace safety.]() * [Introduction]() * [The Imperative of Maritime Safety]() * [Navigation Training: A Cornerstone of Maritime Safety]() * [Mastering Seamanship: A Prerequisite for Safe Sailing]() * [Workplace Safety: Creating a Safe Onboard Environment]() * [Enhancing Skills through Simulation and Practical Training]() * [The Role of Continuous Learning and Professional Development]() * [Conclusion]() Add a header to begin generating the table of contents The Imperative of Maritime Safety --------------------------------- Maritime safety is a multifaceted concept, encompassing various dimensions that collectively contribute to the secure and efficient operation of maritime activities. It is not just about preventing accidents and incidents at sea; it also involves protecting the lives of those on board, safeguarding the marine environment, and ensuring the integrity of the vessel and its equipment. Potential consequences of inadequate training and preparedness in the maritime industry can be severe, ranging from loss of life and damage to property to environmental degradation. Maritime accidents such as collisions, groundings, and fires are often the result of human error, which can be significantly reduced through proper training and skill development. The maritime industry operates in a challenging and often unpredictable environment, making the role of well-trained personnel crucial in navigating these challenges and ensuring safety at sea. International regulations and standards, such as those set by the International Maritime Organization (IMO), play a vital role in promoting maritime safety. These regulations mandate specific training requirements and competency standards for maritime personnel, aiming to create a uniform and high level of safety across the global maritime industry. Compliance with these standards is not just a legal requirement but also a moral imperative, ensuring that every individual who sets foot on a vessel is equipped with the knowledge and skills necessary to perform their duties safely and efficiently. In this context, the imperative of maritime safety becomes clear. It is a shared responsibility, requiring the commitment of individuals, organizations, and regulatory bodies alike. By prioritizing training in navigation, seamanship, and workplace safety, the maritime industry takes a significant step towards minimizing risks, protecting lives, and preserving the marine environment, ultimately contributing to safer seas and a more sustainable future.""",
        """Mariners must be adept at controlling the vessel under various conditions, understanding how it responds to commands, and making precise movements to navigate safely through different maritime environments. **Mooring, Anchoring, and Rope Work:** Effective seamanship also involves skills such as mooring, anchoring, and rope work. These activities require precision, strength, and a deep understanding of the vessel's equipment and capabilities. Proper training ensures that mariners can perform these tasks safely, reducing the risk of accidents and damage to the vessel. **Emergency Manoeuvres and Safety Procedures:** In critical situations, the ability to perform emergency manoeuvres and follow safety procedures can be the difference between safety and disaster. Seamanship training prepares mariners to respond quickly and effectively in emergencies, ensuring that they have the skills and knowledge needed to protect themselves, their crew, and their vessel. **Real-Life Applications and Examples:** The value of seamanship is highlighted through real-life applications and examples, showcasing how these skills have played a crucial role in navigating challenging situations at sea. By learning from these scenarios, mariners gain a deeper appreciation of the importance of seamanship and its impact on maritime safety. Mastering the art of seamanship is a continuous journey, requiring dedication, practice, and a commitment to excellence. By investing in comprehensive seamanship training, the maritime industry ensures that its personnel are equipped with the skills and knowledge necessary to navigate the complexities of the maritime environment, contributing to safer and more efficient maritime operations. Workplace Safety: Creating a Safe Onboard Environment ----------------------------------------------------- The maritime industry presents unique challenges when it comes to ensuring a safe working environment. The isolated nature of work at sea, coupled with the potential for extreme weather conditions and the need for complex machinery operations, makes workplace safety a paramount concern. **Addressing Unique Maritime Challenges:** The onboard environment is vastly different from land-based workplaces, requiring specific safety protocols and training. Mariners must be prepared to handle the challenges of working at sea, from navigating slippery decks to operating heavy equipment in confined spaces. **Implementing Safety Protocols and Emergency Procedures:** A strong safety culture onboard is built on well-established safety protocols and emergency procedures. Regular safety drills, proper use of personal protective equipment, and clear communication channels are essential components of a safe maritime workplace. **Utilizing Safety Management Systems:** The implementation of Safety Management Systems (SMS) onboard vessels is a proactive approach to identifying and mitigating potential risks. These systems provide a structured framework for ensuring safety in all aspects of maritime operations, from navigation to machinery operation and emergency response. **Training and Empowerment:** Empowering mariners with the knowledge and skills to work safely is a critical aspect of workplace safety. This includes training in the proper use of equipment, understanding the risks associated with their work, and knowing how to respond in case of an emergency. **Continuous Improvement and Learning:** A commitment to continuous improvement and learning is essential for maintaining a safe onboard environment. This involves regular reviews of safety procedures, learning from past incidents, and staying updated on best practices in maritime safety. By prioritizing workplace safety and investing in comprehensive training and safety management systems, the maritime industry ensures a safer working environment for all personnel onboard. This not only helps in preventing accidents and injuries but also contributes to the overall efficiency and success of maritime operations. To keep pace with these changes and maintain high standards of safety and efficiency, continuous learning and professional development are essential for maritime personnel. **Adapting to Technological Changes:** The rapid pace of technological advancements in navigation, communication, and ship operations necessitates a commitment to ongoing learning. Mariners must stay abreast of the latest developments, understanding how to operate new equipment and integrate new technologies into their daily routines. **Maintaining and Updating Skills:** The skills required for safe and efficient maritime operations are not static; they need regular updating and refinement. Continuous learning ensures that mariners maintain their skills at the highest level, ready to meet the challenges of modern maritime operations. **Meeting Regulatory Requirements:** The international maritime regulatory landscape is constantly evolving, with new standards and requirements being introduced to enhance safety and environmental protection. Continuous learning ensures that mariners are aware of these changes, understanding their responsibilities and how to comply with the latest regulations. **Fostering a Culture of Safety and Excellence:** A commitment to continuous learning and professional development fosters a culture of safety and excellence within the maritime industry. It sends a clear message that safety is a top priority, and that every individual has a role to play in maintaining high standards of operation. **Enhancing Career Prospects:** For individual mariners, continuous learning and professional development enhance career prospects, opening up opportunities for advancement and specialization. It demonstrates a commitment to their profession and a willingness to invest in their own skills and knowledge. By prioritizing continuous learning and professional development, the maritime industry ensures that its personnel are equipped to handle the challenges of modern maritime operations, maintaining high standards of safety, efficiency, and professionalism. Conclusion ---------- The maritime industry operates in a challenging and unpredictable environment, making the training of personnel in navigation, seamanship, and workplace safety crucial for ensuring safe and efficient operations. The value of such training cannot be overstated, as it directly influences the safety of the vessel, the crew, and the marine environment. Through comprehensive training in navigation, mariners are equipped with the skills necessary to make informed decisions, navigate safely, and respond effectively to navigational challenges. Seamanship training ensures that mariners are adept at handling the vessel, performing crucial maneuvers, and responding to emergencies with precision and confidence. Workplace safety training creates a safe onboard environment, minimizing the risk of accidents and ensuring the well-being of all personnel. Investing in simulation and practical training enhances these skills, providing mariners with hands-on experience and preparing them for the realities of maritime operations. Continuous learning and professional development ensure that these skills remain sharp and up-to-date, adapting to technological advancements and changes in regulations. In conclusion, the value of training in maritime safety is immeasurable. It is a critical investment in the safety and success of maritime operations, fostering a culture of preparedness, excellence, and commitment to safety. By prioritizing training in navigation, seamanship, and workplace safety, the maritime industry takes a significant step towards ensuring safer seas and a more sustainable future. > Ensure the utmost safety and compliance for your marine operations. For expert advice and comprehensive marine safety services, [call us at 508-996-4110]() or [email ](mailto:). Let's prioritize your safety together.""",
        r"""Abstract 1Introduction 2Related work 3Method 4Experiments 5Conclusion References \useunder \ul {CJK} UTF8gbsn \cormark [1] \cortext [cor1] CPLOYO: A Pulmonary Nodule Detection Model with Multi-Scale Feature Fusion and Nonlinear Feature Learning Meng Wang Zi Yang Ruifeng Zhao Yaoting Jiang Department of Radiotherapy, Tongji Hospital, School of Medicine, Tongji University , Shanghai, 200065, China Department of Nuclear Medicine, Shanghai Pulmonology Hospital, School of Medicine, Tongji University , Shanghai, 200065, China Department of Radiotherapy, Shanghai Pulmonology Hospital, School of Medicine, Tongji University, Shanghai, 200065, China Carnegie Mellon University, USA Abstract The integration of Internet of Things (IoT) technology in pulmonary nodule detection significantly enhances the intelligence and real-time capabilities of the detection system. Currently, lung nodule detection primarily focuses on the identification of solid nodules, but different types of lung nodules correspond to various forms of lung cancer. Multi-type detection contributes to improving the overall lung cancer detection rate and enhancing the cure rate. To achieve high sensitivity in nodule detection, targeted improvements were made to the YOLOv8 model. Firstly, the C2f_RepViTCAMF module was introduced to augment the C2f module in the backbone, thereby enhancing detection accuracy for small lung nodules and achieving a lightweight model design. Secondly, the MSCAF module was incorporated to reconstruct the feature fusion section of the model, improving detection accuracy for lung nodules of varying scales. Furthermore, the KAN network was integrated into the model. By leveraging the KAN network's powerful nonlinear feature learning capability, detection accuracy for small lung nodules was further improved, and the model's generalization ability was enhanced. Tests conducted on the LUNA16 dataset demonstrate that the improved model outperforms the original model as well as other mainstream models such as YOLOv9 and RT-DETR across various evaluation metrics. keywords: Lung nodule detection \sepYOLOv8 model \sepC2f_RepViTCAMF \sepMSCAF \sepKAN 1Introduction Lung cancer is the leading cause of mortality from malignant neoplasms worldwide. Despite significant advancements in medical technology, the mortality rate remains high, exerting considerable pressure on global public health systems. Early detection and diagnosis are critical, as timely intervention can significantly improve survival rates [1, 2, 3]. Pulmonary nodules often serve as the first indication of lung cancer. These nodules are typically asymptomatic and can be easily overlooked, yet early detection allows for intervention that greatly enhances the chances of successful treatment. Pulmonary nodules usually appear as circular or oval shadows, ranging from 3 to 30 millimeters in diameter on CT scans. Their variation in shape and size presents a challenge for clinicians, particularly when the nodules are small or irregularly shaped [4, 5].""",
        """CNNs are capable of automatically learning key features during the training process, addressing the limitations of manually designed features, and enhancing the feature extraction capabilities of the network[67, 68, 69]. Currently, deep learning-based lung nodule detection algorithms can be categorized into two main types. One category employs a single-stage algorithm, which directly classifies and regresses the input data through the network model, utilizing the concept of anchor boxes for lung nodule detection. The other category uses a two-stage detection algorithm, where candidate regions are generated first and then detected. Lo et al. developed a dual-matching method and artificial visual neural network technology for lung nodule detection. They first used a spherical template dual-matching technique for high-sensitivity initial tuberculosis search of circular objects, and the artificial CNN served as the final classifier to determine whether suspicious images contained lung nodules, with the total processing time of the method being approximately 15 seconds [70]. U-Net has gained significant popularity due to its outstanding performance in medical image segmentation. Its architecture is based on fully convolutional networks, but with an optimization through the introduction of skip connections [71]. These skip connections allow the network to merge low-level and high-level features at different layers, helping retain more spatial information. Zhou et al. proposed a more robust medical image segmentation architecture, U-Net++, which introduced nested dense skip connections, enabling tighter collaboration between encoder and decoder sub-networks and providing superior performance for medical image segmentation tasks [72]. Zhang et al. proposed a lung nodule detection algorithm based on attention mechanisms and feature pyramids. By using the ResNet backbone network combined with channel-space attention mechanisms, the network extracts more semantic and positional information. In the prediction stage, a feature pyramid network is used to fuse multi-scale features, improving detection performance for small nodules and nodules near blood vessels. Given that lung nodules are typically small in size, usually ranging from 3-6mm, image segmentation can face challenges such as missed detection and blurry boundaries. Consequently, many experts and researchers in the field of lung nodule detection have been continuously working to address these issues. Faster R-CNN employs a two-stage detection method [73]. First, the Region Proposal Network (RPN) extracts candidate bounding boxes, which are then mapped to fixed-size feature maps using Region of Interest (RoI) pooling. Finally, these features are processed by a classifier and a bounding box regressor to perform object classification and location refinement. By incorporating the RPN and adopting a two-stage detection strategy, Faster R-CNN significantly improves the accuracy and efficiency of object detection, making it one of the classic models in the field. El et al. [74] used Faster R-CNN and SSD in the second stage with Inception-V2 as the backbone, achieving a sensitivity of 96.4% on the LUNA16 dataset. Tong et al. introduced an iterative self-organizing data analysis technique into the Faster R-CNN model, reducing false positives with a strategy that utilizes a 3D convolutional neural network to exploit the 3D nature of CT images and addressing class imbalance using focal loss [75]. Setio et al. proposed a multi-view convolutional network, combining candidates obtained from three specialized nodule detectors designed for solid, sub-solid, and large nodules. The network achieved high detection sensitivities of 85.4% and 90.1% with 1 and 4 false positives per scan, respectively [76].""",
        """In the context of pulmonary nodule detection, small objects often lack clear, distinct features and are surrounded by complex backgrounds, making them hard to identify. To tackle this, the Neck module focuses on effective feature extraction and multi-scale feature fusion. The integration of the Feature Pyramid Network (FPN) enables the model to process features at different resolutions, ensuring both small and large objects are accurately detected. Additionally, the incorporation of the KAN-Bottleneck further optimizes the feature extraction process, particularly in low-contrast or imbalanced datasets, enabling the model to better handle the detection of small targets, such as pulmonary nodules. In addition to FPN, CPYOLO integrates a KAN-Bottleneck to improve the feature extraction process, as shown in Figure4. The Bottleneck module is crucial for reducing the dimensionality of feature maps and subsequently restoring them, which reduces computational cost while increasing network depth. The KAN-Bottleneck replaces traditional convolutional layers with the KAN network, which avoids the linear combination of inputs and directly applies nonlinear activation functions to the input data. This design enhances the model's ability to learn complex feature mappings, particularly when dealing with small targets such as pulmonary nodules. Figure 4:Structure Diagram of KAN_BottleNeck Module The KAN network's design enables better feature learning through its learnable univariate activation functions and external function summation. This increases its flexibility and efficiency, especially when training samples are limited or the dataset is imbalanced. The mathematical expression for the KAN network is as follows: ( ) = = , ( ) (Summation of internal functions) where , ( ) represents the learnable internal functions, and the summation of external functions is given by: ( ) = = Φ ( ) (Summation of external functions) By incorporating KAN-Bottleneck, the CPYOLO model enhances the extraction of refined features, improving detection accuracy for small pulmonary nodules. Furthermore, CPYOLO integrates two Convolutional Block Attention Modules (CBAM) within the Neck module to further enhance feature extraction. The CBAM modules apply attention mechanisms in both channel and spatial dimensions, focusing on the most relevant features. The channel attention mechanism assigns weights to channels based on their importance, while the spatial attention mechanism highlights critical regions in the input feature maps. These attention mechanisms allow the model to prioritize important features and suppress irrelevant ones. The CBAM module consists of two sub-modules: the Channel Attention Mechanism and the Spatial Attention Mechanism. The Channel Attention Mechanism operates by applying global average pooling (GAP) and global max pooling (GMP) to the input feature map , producing two spatial representation vectors. These vectors are processed by a Multi-Layer Perceptron (MLP) to generate the final channel-wise attention weights. The mathematical expression for the channel attention mechanism is as follows: ( ) = ( MLP ( AvgPool ( ) ) + MLP ( MaxPool ( ) ) ) where denotes the input feature map, AvgPool ( ) represents global average pooling, MaxPool ( ) represents global max pooling, MLP is the multi-layer perceptron, and is the Sigmoid activation function. The Spatial Attention Mechanism highlights important spatial regions in the input feature map. It begins by applying GAP and GMP along the channel dimension to generate two separate feature maps. These maps are concatenated and passed through a convolutional layer with a 7x7 kernel to produce a spatial attention map. This map is then processed by a Sigmoid activation function to generate the final spatial attention feature map. The mathematical expression for the spatial attention mechanism is as follows: ( ) = ( × ( [ AvgPool ( ) ; MaxPool ( ) ] ) ) In this equation, AvgPool ( ) and MaxPool ( ) represent global average pooling and global max pooling operations, respectively, and × is the convolutional kernel applied to the concatenated pooled features. By combining the KAN-Bottleneck and CBAM modules, CPYOLO effectively enhances its ability to focus on important features and suppress irrelevant ones, leading to improved detection performance, especially in tasks such as pulmonary nodule detection. 3.3Transfer learning In pulmonary nodule detection, ensuring the generalization and stability of the model is crucial, which requires extensive and diverse training data. Typesetting math: 100% zoom_out_map menu Search: Diagnostics All Article Types Advanced Journals Diagnostics Volume 14 Issue 22 10.3390/diagnostics share Share announcement Help format_quote Cite question_answer Discuss in SciProfiles settings Order Article Reprints Open AccessArticle A Novel Method for 3D Lung Tumor Reconstruction Using Generative Models by Hamidreza Najafi 1,Kimia Savoji 2,Marzieh Mirzaeibonehkhater 3,Seyed Vahid Moravvej 4,Roohallah Alizadehsani 5 andSiamak Pedrammehr 6,* Biomedical Engineering Department, School of Electrical Engineering, Iran University of Science and Technology, Tehran 16846-13114, Iran Biomedical Data Science and Informatics, School of Computing, Clemson University, Clemson, SC 29634, USA Department of Electrical and Computer Engineering, Indiana University-Purdue University, Indianapolis, IN 46202, USA Department of Electrical and Computer Engineering, Isfahan University of Technology, Isfahan 84156-83111, Iran Institute for Intelligent Systems Research and Innovation (IISRI), Deakin University, Geelong, VIC 3216, Australia Faculty of Design, Tabriz Islamic Art University, Tabriz 51647-36931, Iran * Author to whom correspondence should be addressed. Diagnostics 2024, 14(22), 2604; Submission received: 11 September 2024 / Revised: 2 November 2024 / Accepted: 12 November 2024 / Published: 20 November (This article belongs to the Special Issue AI and Digital Health for Disease Diagnosis and Monitoring) Download keyboard_arrow_down Browse Figures VersionsNotes Abstract Background: Lung cancer remains a significant health concern, and the effectiveness of early detection significantly enhances patient survival rates. Identifying lung tumors with high precision is a challenge due to the complex nature of tumor structures and the surrounding lung tissues. Methods: To address these hurdles, this paper presents an innovative three-step approach that leverages Generative Adversarial Networks (GAN), Long Short-Term Memory (LSTM), and VGG16 algorithms for the accurate reconstruction of three-dimensional (3D) lung tumor images. The first challenge we address is the accurate segmentation of lung tissues from CT images, a task complicated by the overwhelming presence of non-lung pixels, which can lead to classifier imbalance.""",
        """Through this research, we aim to showcase the application of deep learning in medical imaging and its potential to transform early detection and treatment. A variety of methods have been developed for lung segmentation, which includes the use of hand-selected features and the careful tuning of experimental parameters[5]. However, these methods are often explicitly tailored for particular applications and datasets, limiting their generalizability, especially in medical imaging data like CT scans. This limitation is further exacerbated by the tendency to overlook certain features during the feature selection phase[6,7], as practitioners often skip detailed feature selection in favor of more intuitive interactive methods. Traditional machine learning approaches typically adopt a two-step process, beginning with feature selection and then applying a learning algorithm to build an automated system. Initially, experts choose or develop a method specifically for feature selection tailored to the task at hand[8]. In contrast, deep learning models integrate feature selection with the learning process, eliminating the need for separate feature selection algorithms[9,10,11,12]. Deep learning has been particularly effective in lung segmentation, focusing on classifying each pixel as either lung tissue or not. However, this pixel-wise classification approach is challenged by class imbalance, as the number of non-lung pixels significantly outnumbers lung pixels. To address the class imbalance issue, strategies are employed at both the data handling and algorithmic levels[13,14]. Regarding data handling, techniques include downsampling the overrepresented classes, upsampling the underrepresented classes, or generating synthetic data to balance the dataset. At the algorithmic level, efforts focus on adjusting algorithms to recognize better and weigh the contributions of the less represented classes, which is essential for accurate predictions[15]. The primary problem with data-driven methods lies in their potential to reflect and amplify existing biases in the data, leading to a preference for more common outcomes and overlooking rare but essential cases[16]. From an algorithmic standpoint, the challenge is to adapt learning models to capture these vital, albeit infrequent, phenomena, thereby improving the accuracy and reliability of the model. The advent of RL has garnered considerable attention for its capability to address complex challenges, especially in scenarios marked by class imbalances[17].""",
        """The inclusion of RL is driven by its ability to handle the complex, dynamic environments typical of medical imaging data, especially when dealing with imbalanced datasets such as lung CT images, where non-lung pixels vastly outnumber lung pixels. RL operates on agents learning to make decisions by performing actions in an environment to achieve cumulative rewards. In our GAN model, the generator acts as an agent that receives feedback from the discriminator (environment), which assesses the generated lung segmentation against the real data. The RL approach allows the generator to learn more nuanced strategies for producing realistic segmentations, beyond mere imitation of the training data, by rewarding the generator for segmenting lung regions that challenge the discriminator to distinguish from real lung tissues. The decision to use RL in training the GAN model is motivated by the need to address the class imbalance problem effectively. Traditional training methods might lead the model to overlook minority classes (lung pixels) due to their lesser presence in the training data. RL addresses this by assigning higher rewards for correctly identifying lung pixels, encouraging the model to pay more attention to these critical areas. This method enhances the sensitivity of the model to lung regions, improving segmentation accuracy and, consequently, the reliability of subsequent tumor detection and 3D reconstruction. The theoretical implications of our research are significant, marking a substantial advancement in medical imaging and automated disease diagnosis. By integrating state-of-the-art machine learning techniques such as GANs, LSTM networks, and the VGG16 algorithm, our model sets a new benchmark in lung tumor detection and 3D reconstruction. Using GANs for segmentation and reconstruction introduces an innovative approach that leverages their generative capabilities to produce highly accurate depictions of lung tumors. Furthermore, applying LSTM networks to process sequential CT scan data underscores the potential of recurrent neural networks in preserving and utilizing temporal information, which is essential for constructing accurate 3D tumor models. Incorporating the VGG16 algorithm enhances the ability of the model to extract detailed and relevant features from complex medical images, laying a solid foundation for the accurate segmentation and detection of lung tumors. This research contributes to improving lung cancer diagnostic processes. It provides a versatile framework that could be adapted for other types of cancer and diseases, showcasing these advanced machine-learning techniques' broad applicability and impact in healthcare. The limitations of the proposed model are as follows: Data diversity and availability: One of the main limitations of our model is its dependence on the availability and diversity of training data. Medical imaging datasets, especially those for lung cancer, can vary widely in terms of quality, resolution, and the types of imaging equipment used. This variability can affect the ability of the model to generalize across clinical settings and patient populations.""",
        """ Treatise on Geophysics: Mantle Dynamics, Volume 7 aims to provide both a classical and state-of-the-art introduction to the methods and science of mantle dynamics, as well as survey leading order problems (both solved and unsolved) and current understanding of how the mantle works. It is organized around two themes: (1) how is mantle convection studied; and (2) what do we understand about mantle dynamics to date. The first four chapters are thus concerned with pedagogical reviews of the physics of mantle convection; laboratory studies of the fluid dynamics of convection relevant to the mantle; theoretical analysis of mantle dynamics; and numerical analysis and methods of mantle convection. The subsequent chapters concentrate on leading issues of mantle convection itself, which include the energy budget of the mantle; the upper mantle and lithosphere in and near the spreading center (mid-ocean ridge) environment; the dynamics of subducting slabs; hot spots, melting anomalies, and mantle plumes; and finally, geochemical mantle dynamics and mixing.Self-contained volume starts with an overview of the subject then explores each topic in detailExtensive reference lists and cross references with other volumes to facilitate further researchFull-color figures and tables support the text and aid in understandingContent suited for both the expert and non-expert""",
        """The growing market penetration of Internet mapping, satellite imaging and personal navigation has opened up great research and business opportunities to geospatial communities. Multi-platform and multi-sensor integrated mapping technology has clearly established a trend towards fast geospatial data acquisition. Sensors can be mounted on various pla""",
        """Job Hazard Analysis: A Guide for Voluntary Compliance and Beyond presents a new and improved concept for Job Hazard Analysis (JHA) that guides the reader through the whole process of developing tools for identifying workplace hazards, creating systems that support hazard recognition, designing an effective JHA, and integrating a JHA based program into occupational safety and health management systems. The book goes beyond the traditional approach of focusing just on the sequence of steps and demonstrates how to integrate a risk assessment and behavioral component into the process by incorporating elements from Behavior-Related Safety and Six Sigma. This approach allows businesses to move from mere compliance to pro-active safety management. This book methodically develops the risk assessment basis needed for ANSI/AIHA Z10 and other safety and health management systems. It is supported by numerous real-life examples, end of chapter review questions, sample checklists, action plans and forms. There is a complete online solutions manual for instructors adopting the book in college and university occupational safety and health courses. This text is intended for lecturers and students in occupational safety and health courses as well as vocational and degree courses at community colleges and universities. It will also appeal to safety and health professionals in all industries; supervisors, senior managers and HR professionals with responsibility for safety and health; and loss control and insurance professionals.Enhances the JHA with concepts from Behavior- Related Safety and proven risk assessment strategies using Six Sigma toolsMethodically develops the risk assessment basis needed for ANSI/AIHA Z10 and other safety and health management systemsIncludes numerous real-life examples, end of chapter review questions, sample checklists, action plans and forms""",
        """Astronomy is an observational science, renewed and even revolutionized by new developments in instrumentation. With the resulting growth of multiwavelength investigation as an engine of discovery, it is increasingly important for astronomers to understand the underlying physical principles and operational characteristics for a broad range of instruments. This comprehensive text is ideal for graduate students, active researchers and instrument developers. It is a thorough review of how astronomers obtain their data, covering current approaches to astronomical measurements from radio to gamma rays. The focus is on current technology rather than the history of the field, allowing each topic to be discussed in depth. Areas covered include telescopes, detectors, photometry, spectroscopy, adaptive optics and high-contrast imaging, millimeter-wave and radio receivers, radio and optical/infrared interferometry, and X-ray and gamma-ray astronomy, all at a level that bridges the gap between the basic principles of optics and the subject's abundant specialist literature. Color versions of figures and solutions to selected problems are available online at www.cambridge.org/9780521762298.""",
        """Typesetting math: 100% zoom_out_map menu Search: Diagnostics All Article Types Advanced Journals Diagnostics Volume 14 Issue 22 10.3390/diagnostics share Share announcement Help format_quote Cite question_answer Discuss in SciProfiles settings Order Article Reprints Open AccessArticle A Novel Method for 3D Lung Tumor Reconstruction Using Generative Models by Hamidreza Najafi 1,Kimia Savoji 2,Marzieh Mirzaeibonehkhater 3,Seyed Vahid Moravvej 4,Roohallah Alizadehsani 5 andSiamak Pedrammehr 6,* Biomedical Engineering Department, School of Electrical Engineering, Iran University of Science and Technology, Tehran 16846-13114, Iran Biomedical Data Science and Informatics, School of Computing, Clemson University, Clemson, SC 29634, USA Department of Electrical and Computer Engineering, Indiana University-Purdue University, Indianapolis, IN 46202, USA Department of Electrical and Computer Engineering, Isfahan University of Technology, Isfahan 84156-83111, Iran Institute for Intelligent Systems Research and Innovation (IISRI), Deakin University, Geelong, VIC 3216, Australia Faculty of Design, Tabriz Islamic Art University, Tabriz 51647-36931, Iran * Author to whom correspondence should be addressed. Diagnostics 2024, 14(22), 2604; Submission received: 11 September 2024 / Revised: 2 November 2024 / Accepted: 12 November 2024 / Published: 20 November (This article belongs to the Special Issue AI and Digital Health for Disease Diagnosis and Monitoring) Download keyboard_arrow_down Browse Figures VersionsNotes Abstract Background: Lung cancer remains a significant health concern, and the effectiveness of early detection significantly enhances patient survival rates. Identifying lung tumors with high precision is a challenge due to the complex nature of tumor structures and the surrounding lung tissues. Methods: To address these hurdles, this paper presents an innovative three-step approach that leverages Generative Adversarial Networks (GAN), Long Short-Term Memory (LSTM), and VGG16 algorithms for the accurate reconstruction of three-dimensional (3D) lung tumor images. The first challenge we address is the accurate segmentation of lung tissues from CT images, a task complicated by the overwhelming presence of non-lung pixels, which can lead to classifier imbalance. Our solution employs a GAN model trained with a reinforcement learning (RL)-based algorithm to mitigate this imbalance and enhance segmentation accuracy. The second challenge involves precisely detecting tumors within the segmented lung regions. We introduce a second GAN model with a novel loss function that significantly improves tumor detection accuracy. Following successful segmentation and tumor detection, the VGG16 algorithm is utilized for feature extraction, preparing the data for the final 3D reconstruction. These features are then processed through an LSTM network and converted into a format suitable for the reconstructive GAN. This GAN, equipped with dilated convolution layers in its discriminator, captures extensive contextual information, enabling the accurate reconstruction of the tumor's 3D structure. Results: The effectiveness of our method is demonstrated through rigorous evaluation against established techniques using the LIDC-IDRI dataset and standard performance metrics, showcasing its superior performance and potential for enhancing early lung cancer detection. study highlights the benefits of combining GANs, LSTM, and VGG16 into a unified framework. This approach significantly improves the accuracy of detecting and reconstructing lung tumors, promising to enhance diagnostic methods and patient results in lung cancer treatment. Keywords: lung cancer; lung segmentation; tumor detection; 3D tumor reconstruction; generative adversarial network; imbalanced data 1. Introduction Lung cancer stands as a critical global health challenge, marking it as one of the deadliest cancers that claim the lives of millions worldwide annually, regardless of gender. For example, there were about 2.09 million new cases in 2018, leading to nearly 1.76 million deaths[1]. Detecting the disease early can substantially improve the chances of survival. However, despite the availability of sophisticated imaging tools such as CT scans, X-rays, and magnetic resonance imaging (MRI), inherent limitations hinder their effectiveness. CT scans offer detailed three-dimensional views of lung tissue; however, their high rates of false positives can lead to unnecessary biopsies and surgeries[2]. Although X-rays are commonly used for initial screenings, they lack the sensitivity to detect early-stage tumors and struggle to differentiate between benign and malignant nodules. MRIs deliver excellent contrast in soft tissues, yet their use in lung imaging is limited due to the lungs' high air content and motion during breathing[3]. Integrating deep learning in medical imaging presents a promising frontier for overcoming these challenges. Deep learning models, trained on extensively annotated image datasets, have demonstrated remarkable capabilities in identifying subtle patterns that elude human radiologists and traditional machine learning models. These models can enhance the specificity and sensitivity of lung cancer detection, reduce the rate of false positives, and potentially decrease the need for invasive diagnostic procedures. In this article, we present a sophisticated deep-learning model tailored to construct the 3D structures of lung tumors accurately. This model leverages deep learning's strengths to surpass the limitations of existing imaging technologies. The 3D lung tumor reconstruction process unfolds in three primary steps: lung segmentation, tumor identification and extraction, and reconstructing the tumor's 3D shape using sequential images. This methodology sharpens the clarity of the tumor's spatial configuration and provides crucial information that aids effective medical decision-making and treatment planning[4]. Through this research, we aim to showcase the application of deep learning in medical imaging and its potential to transform early detection and treatment. A variety of methods have been developed for lung segmentation, which includes the use of hand-selected features and the careful tuning of experimental parameters[5]. However, these methods are often explicitly tailored for particular applications and datasets, limiting their generalizability, especially in medical imaging data like CT scans. This limitation is further exacerbated by the tendency to overlook certain features during the feature selection phase[6,7], as practitioners often skip detailed feature selection in favor of more intuitive interactive methods. Traditional machine learning approaches typically adopt a two-step process, beginning with feature selection and then applying a learning algorithm to build an automated system. Initially, experts choose or develop a method specifically for feature selection tailored to the task at hand[8]. In contrast, deep learning models integrate feature selection with the learning process, eliminating the need for separate feature selection algorithms[9,10,11,12]. Deep learning has been particularly effective in lung segmentation, focusing on classifying each pixel as either lung tissue or not. However, this pixel-wise classification approach is challenged by class imbalance, as the number of non-lung pixels significantly outnumbers lung pixels. To address the class imbalance issue, strategies are employed at both the data handling and algorithmic levels[13,14]. Regarding data handling, techniques include downsampling the overrepresented classes, upsampling the underrepresented classes, or generating synthetic data to balance the dataset. At the algorithmic level, efforts focus on adjusting algorithms to recognize better and weigh the contributions of the less represented classes, which is essential for accurate predictions[15]. The primary problem with data-driven methods lies in their potential to reflect and amplify existing biases in the data, leading to a preference for more common outcomes and overlooking rare but essential cases[16]. From an algorithmic standpoint, the challenge is to adapt learning models to capture these vital, albeit infrequent, phenomena, thereby improving the accuracy and reliability of the model. The advent of RL has garnered considerable attention for its capability to address complex challenges, especially in scenarios marked by class imbalances[17]. RL enhances the efficacy of classification models by filtering out irrelevant details and emphasizing crucial features, demonstrating versatility across various domains. A notable strength of RL is its adaptive learning approach, guided by reward mechanisms, which is particularly effective in managing data imbalance issues[18]. By tailoring the reward system to prioritize accurately identifying underrepresented classes, RL strategies can focus more on these often-neglected segments. This method promotes a more equitable distribution of attention across different courses, significantly improving detection accuracy[19]. The 3D reconstruction from CT images has been thoroughly explored, significantly relying on statistical methodologies[20,21,22]. While these approaches offer considerable value, they encounter several notable limitations. One of the primary challenges is the dependency on extensive annotated datasets, which are notably difficult to obtain in medical imaging due to stringent privacy regulations and the necessity for annotations by medical experts. Additionally, these methods have a heightened risk of overfitting, compromising their ability to be applied effectively across varied medical datasets[23]. The assumptions made about data distribution in statistical methods may only sometimes align with the realities of complex medical environments, potentially leading to reconstruction inaccuracies[24]. The inherently high dimensionality of medical imaging data also presents computational hurdles, often requiring dimensionality reduction techniques that risk excluding crucial information. Moreover, statistical approaches may need help to accurately represent the intricate spatial relationships and textures, which is essential for precise 3D reconstructions in medical imaging[25]. GANs[26] present a transformative approach to overcoming the limitations inherent to traditional statistical methods for the 3D reconstruction of CT images, leveraging their distinctive architecture and learning paradigms. Central to the GAN framework is the symbiotic relationship between two deep neural networks: the generator and the discriminator[27]. The role of the generator is to create synthetic yet plausible data that mimic real CT images, whereas the discriminator evaluates these generated samples, striving to differentiate between the authentic and the synthetic[28]. This adversarial dynamic fosters a continuous learning cycle, driving both networks toward higher performance levels. The generator learns to produce increasingly realistic images, refining details to deceive the discriminator, while the discriminator enhances its ability to detect the subtleties that distinguish real images from fakes. This process generates high-quality, realistic 3D reconstructions that can be difficult to differentiate from actual CT scans. This study presents an innovative triple-layered methodology for generating three-dimensional models of lung cancer from CT scans. The process begins with segmenting the CT images using a GAN incorporating the U-net architecture, enabling precise pixel-level analysis. To tackle the class imbalance issue, we utilize an RL algorithm that prefers less represented classes, enhancing the sensitivity of the model to rarer but crucial features. The discriminator network, equipped with dilated convolution layers, improves the quality of the generator's output by effectively differentiating between authentic and synthesized images and emphasizing key features. In the subsequent phase, we employ another GAN setup with the Mask R-CNN framework, which accurately identifies and localizes affected regions within the segmented images, thereby facilitating precise tumor detection and classification. To further refine the accuracy of these identified areas, an error correction component is integrated into the generator's loss function, and the discriminator continues to use dilated convolution layers for high-quality output validation. The final stage of the process involves 3D modeling, where LSTM networks combined with GAN technology play a pivotal role. Here, features extracted from two-dimensional pulmonary scans by a trained VGG16 network are further processed by an LSTM network, which employs an attention mechanism to highlight critical tumor-related features. The generator then creates a three-dimensional representation based on the output of the LSTM. At the same time, the discriminator evaluates the fidelity of these 3D models against real images, ensuring the reconstructed models closely mirror actual lung structures. The efficacy of our method is validated through comprehensive testing, benchmarked against well-established approaches using the LIDC-IDRI dataset[29] and conventional performance metrics. This comparison highlights our method's superior capabilities and promising potential to improve the early detection of lung cancer. The proposed model in this study offers several significant contributions to the field of medical imaging, specifically in the 3D reconstruction of lung cancer from CT scans: Addressing class imbalance with RL: Traditional methods for lung segmentation often need help with class imbalance, where certain lung conditions are significantly underrepresented in training datasets. Our approach integrates RL to target this issue specifically. Using RL, the model dynamically adjusts its focus towards these less-represented classes during training, enhancing its sensitivity and specificity in these areas. This targeted focus mitigates the class imbalance problem and improves the model's generalization capabilities across diverse patient datasets. Innovative reward mechanism in RL for enhanced lung segmentation: We have developed an innovative reward mechanism within our RL strategy tailored for lung segmentation. This mechanism provides a higher reward for correctly identifying typically underrepresented classes. This approach ensures that the model pays greater attention to these infrequent classes, promoting a more equitable and balanced segmentation process. Utilization of dilated convolution in discriminator network: Our model incorporates a discriminator network that employs dilated convolution layers. This architectural choice allows the network to expand its receptive field without losing resolution, enabling it to better differentiate between real and synthesized images. The ability to focus on more extensive and critical features significantly improves the reconstructed images' visual clarity and detail resolution. Error correction in generator's loss function for tumor detection: We have integrated an error correction component into the loss function of our generator during the tumor detection phase. This component is specifically designed to fine-tune the detection accuracy and minimize potential errors in the output. By implementing this correction process, the model enhances its reliability and ensures high precision in tumor detection. The remainder of the article is organized as follows: Section 2 discusses the related work, Section 3 details the proposed methodology, Section 4 describes the experiments conducted, and Section 5 provides the conclusion and outlines future research directions. 2. Related Work The literature review is divided into three sections: lung segmentation, tumor detection, and 3D tumor reconstruction. 2.1. Lung Segmentation Hasan et al.[30] delve into deep learning applications for diagnosing diseases such as COVID-19 and tuberculosis by analyzing pulmonary X-rays, explicitly employing the deeplabv3plus CNN-based semantic segmentation model that utilizes "Atrous Convolution" to preserve feature resolution in deeper layers of the network. Swaminathan et al.[31] tackle the pressing issue of diagnosing lung cancer at advanced stages by employing deep learning to improve early detection rates, particularly in high-risk individuals. Their methodology involves initial preprocessing of CT images with the Wiener filter, followed by segmentation using a GAN trained with the Salp Shuffled Shepherd Optimization Algorithm (SSSOA), and subsequent classification via the VGG16 CNN model. Gite et al.[32] present an innovative approach to lung segmentation in X-ray images using U-Net++, aiming to boost tuberculosis detection rates. This novel method surpasses traditional techniques and mitigates data leakage problems, significantly enhancing diagnostic precision. Irawan et al.[33] unveil a modified U-Net architecture tailored for semantic lung segmentation from chest X-rays, integrating multiple dropouts in deconvolutional layers to avoid overfitting, thereby ensuring high accuracy and model generalization with a streamlined framework. Rehman et al.[34] validate the U-Net framework's capability in segmenting lung areas from X-ray images, attaining a notable average Intersection over Union (IoU) score of 92.82. Shaoyong Guo et al.[35] introduce a sophisticated automated segmentation method that merges radionics with manually selected and algorithmically derived features, achieving a Dice similarity index of 89.42% on the ILD database MedGIFT. Chen Zhou et al.[36] develop an advanced automatic segmentation system that incorporates a 3D V-Net and a Spatial Transform Network (STN) to accurately segment lung tissue in CT scans, facilitating the diagnosis of COVID-19 by examining textures and other critical features. Mizuho Nishio et al.[37] utilize a U-Net framework optimized through Bayesian methods on datasets from Japan and Montgomery, securing high Dice Similarity Coefficients (DSC) of 0.976 and 0.973, respectively. Ferreira et al.[38] propose a unique U-Net variant designed for the automated detection of COVID-19 infections in CT scans, validated on actual patient data from Pedro Ernesto University Hospital in Rio de Janeiro. Feidao Cao[39] enhances the conventional U-Net structure by incorporating a Variational Auto-Encoder (VAE) in each encoder-decoder layer, markedly improving the network's feature extraction efficiency. This advanced network is evaluated using NIH and JRST datasets, demonstrating exceptional accuracy and F1 scores of 0.9701 and 0.9334 for the former and 0.9750 and 0.9578 for the latter, respectively. Many current lung segmentation techniques involve classifying each pixel as either lung tissue or not, which presents a challenge due to class imbalance; the overwhelming majority of pixels represent non-lung tissue. Acknowledging this issue, our paper introduces an RL-based innovative approach to lung segmentation that circumvents the problem of unbalanced classification. 2.2. Tumor Detection Vijh et al.[40] introduced a hybrid bio-inspired algorithm named WOA_APSO, which merges the whale optimization algorithm (WOA) with adaptive particle swarm optimization (APSO) to refine feature selection in a CNN for lung CT image classification, thereby improving tumor detection accuracy. Rathod et al.[41] developed DLCTLungDetectNet, a deep learning framework that employs a CNN with FusionNet, integrating features from ResNet50 and InceptionV3, to enhance early lung cancer detection in CT scans. Their model outperforms traditional architectures like VGG16 and Inception v3, setting a new standard in automated lung tumor detection. Venkatesh et al.[42] proposed a novel method for early lung cancer detection in CT images using patch processing and deep learning to classify tumors efficiently, aiming for accurate detection with less computational time and improved performance metrics. Sundarrajan et al.[43] introduced an advanced lung tumor detection approach leveraging cloud-based IoT for data collection and employing an optimized fuzzy C-means neural network (OFCMNN) for segmentation, followed by classification with a kernel multilayer deep transfer convolutional learning (KM-DTCL) ensemble to provide superior diagnostic performance. Srinivasulu et al.[44] developed a deep learning model, ECNN-ERNN with autoencoders, for early detection of lung malignancies in CT images, utilizing a modified framework to achieve high accuracy and insightful feature extraction. Manickavasagam et al.[45] proposed CNN-5CL. This novel deep learning methodology uses an 11-layer CNN, including five convolutional layers, for precise pulmonary nodule classification in CT images, benchmarked with LIDC/IDRI datasets. Agarwal et al.[46] introduced a method that combines a CNN with the AlexNet Network Model for early lung tumor classification, distinguishing malignant from benign tumors with greater accuracy and addressing the challenge of premature lung cancer diagnosis. Li and Fan[47] unveiled a 3D CNN-based pulmonary nodule detection model featuring an encoder-decoder architecture, a dynamically scaled cross-entropy method for reducing false positives, and a squeeze-and-excitation structure to capitalize on channel dependencies. 2.3. Three-Dimensional Tumor Reconstruction Recent advancements in the medical field have led to the development of 3D models, significantly enhancing patient treatment strategies[48,49]. A prime example of this application is in liver resection surgeries, where 3D models are crucial in providing surgeons with a detailed understanding of liver anatomy[50]. Furthermore, the creation of 3D brain models using magnetic resonance imaging (MRI) has been investigated[51]. The predominant emphasis in current research revolves around the 3D reconstruction of lung tumors[52,53]. Afshar et al.[21] proposed a method for segmenting lung cancer tumors and reconstructing 3D CT images. Their approach used snake optimization for lung segmentation, Gustafson and Kessel (GK) clustering[54] for tumor detection, and a statistical technique for 3D reconstruction. Hong et al.[22], and Rezaei et al.[55] employed a similar method for lung and tumor detection but with including a GAN-based approach for reconstructing 3D images. However, the accuracy of snake optimization heavily relies on the initial positioning of the contour[56]. If the initial contour placement is correct or deviates from the actual lung boundaries, it can lead to accurate segmentation results. Snake optimization may face challenges when segmenting complex lung structures, such as irregular shapes, nodules, or lesions, as these variations can make it difficult for the contour to adapt and accurately capture their boundaries. Similarly, GK clustering, like other clustering algorithms, is sensitive to the initial selection of cluster centroids. Choosing appropriate initial centroids is crucial to obtain accurate tumor detection. Poorly chosen centroids can cause suboptimal clustering outcomes. GK clustering may also struggle to handle cases where tumors overlap or exhibit heterogeneous characteristics, making it challenging to differentiate between different tumor regions and assign data points accurately to their respective clusters[52]. 3. The Proposed Model Figure 1 illustrates the overall structure of the proposed model, which encompasses three key steps: lung segmentation, tumor detection, and 3D tumor reconstruction. In the subsequent sections, we will discuss each step, elaborating on their functionalities andprocesses. Figure 1. Overview of the proposed model: In step 1, two lungs are segmented in CT images using the GAN-based model. In step 2, the tumor is detected using the second GAN-based model. After the features are extracted by VGG16, a 3D model of the tumor is reconstructed using the third GAN in step 3. 3.1. Lung Segmentation The primary objective of segmentation in medical imaging is to produce precise lung outlines that closely mirror the reference or original outline. These outlines are pivotal for many medical uses and further examinations by accurately defining the edges of the lungs. To achieve this, we utilize a duo of neural networks: a generator (G) and a discriminator (D). G is crucial in crafting lung outlines, taking black-and-white CT scans as input. Through its complex structure, G is adept at identifying significant features and patterns within the images, creating outlines that suggest the areas of the lungs. These outlines endeavor to accurately capture the complex shapes and perimeters of the lungs, which is crucial for detailed segmentation. Conversely, D collaborates with G to boost the precision and quality of these outlines. Acting as a rigorous assessor, D examines how closely the created outlines match the reference or original outlines. Utilizing sophisticated methods like the Earth Mover's (EM) distance, D measures the variance between the predicted and the reference outlines, enhancing G's output. This motivates G to produce outlines that align with the original, improving the segmentation outcomes. During the training phase, G is fed a lung CT scan image, referred to as , and generates a corresponding outline that highlights the areas of the lungs. D then scrutinizes the created outline for its fidelity and quality, offering critical feedback that aids G's learning and advancement. This dynamic interaction between G and D, coupled with reference outlines, aids in continuously enhancing the lung outlines generated through the training. In the forthcoming sections, we will explore the intricacies of G and D's designs, elucidating their structure and functionalities. We will also discuss the loss function utilized during training, which is significant in steering the networks toward yielding more precise and dependable lung segmentation. 3.1.1. Generator Architecture In our context, segmentation is treated as a task where each pixel in an image is classified as either part of the lung area or not. To perform this task, we rely on the precision of a U-Net-based generator network, as depicted in Figure 2. When processing a CT scan labeled , the generator network (G) meticulously evaluates each pixel to determine its association with the lung region, subsequently generating a mask that accurately reflects this classification. The generator network has two principal components: the encoder and the decoder. The encoder, in its complex task, begins by taking the input image and, through a series of convolution layers, extracts features at multiple scales. These layers enable the encoder to delve into the image, capturing a broad range of details and complex information from the image. Following the feature extraction, the decoder, playing a pivotal role, uses these multi-level features to construct the masks. By leveraging these features, the decoder can accurately reconstruct the intricate structures and contours of the lung area, resulting in detailed and precise masks. Both the encoder and decoder sections include convolution layers, which are crucial in neural network designs for identifying local patterns and spatial correlations, and are key elements for accurate segmentation. Figure 2. Architecture of the U-Ne-based generator network used for lung segmentation, illustrating the flow from input CT scan through the encoder and decoder stages to the final mask output. The generator network is designed as a binary pixel classifier, identifying each pixel as either part of the lung area (labeled as one) or not (labeled as zero). A significant challenge arises due to the overwhelming number of pixels labeled as zero, which is not associated with the lung area and leads to an imbalance. This imbalance causes the classifier to lean towards predicting pixels as non-lung areas, which can drastically reduce the model's ability to delineate the lung region accurately. To counteract this issue, the training strategy for the generator incorporates an RL algorithm. RL is based on reward and punishment mechanisms, making it an ideal solution for addressing the problem of imbalanced classification. Within this framework, the RL algorithm is customized to assign much higher rewards for accurately identifying pixels within the minority class (the lung region) than those for correctly classifying pixels in the majority class (non-lung area). Conversely, the penalties for incorrectly classifying lung region pixels are more stringent than errors made within the majority class. Despite their scarcity, this strategy shifts the model's focus towards more accurately recognizing and classifying pixels within the lung region. By amplifying the consequences for the minority class, the RL algorithm motivates the generator network to concentrate more on lung areas, thus mitigating the bias towards the more prevalent non-lung pixels.""",
        """Go to Environment International on ScienceDirect") [![Image 10: Environment International]()]() Under a Creative Commons [license]() open access Abstract -------- Accurate air quality forecasting is crucial for public health, environmental monitoring and protection, and urban planning. However, existing methods fail to effectively utilize multi-scale information, both spatially and temporally. There is a lack of integration between individual monitoring stations and city-wide scales. Temporally, the periodic nature of air quality variations is often overlooked or inadequately considered. To overcome these limitations, we conduct a thorough analysis of the data and tasks, integrating spatio-temporal multi-scale domain knowledge. We present a novel **M**ulti-spatial **M**ulti-temporal air quality forecasting method based on **G**raph Convolutional Networks and **G**ated Recurrent Units (M2G2), bridging the gap in air quality forecasting across spatial and temporal scales. The proposed framework consists of two modules: Multi-scale Spatial GCN (MS-GCN) for spatial information fusion and Multi-scale Temporal GRU (MT-GRU) for temporal information integration. In the spatial dimension, the MS-GCN module employs a bidirectional learnable structure and a residual structure, enabling comprehensive information exchange between individual monitoring stations and the city-scale graph. Regarding the temporal dimension, the MT-GRU module adaptively combines information from different temporal scales through parallel hidden states. Leveraging meteorological indicators and four air quality indicators, we present comprehensive comparative analyses and ablation experiments, showcasing the higher accuracy of M2G2 in comparison to nine currently available advanced approaches across all aspects. The improvements of M2G2 over the second-best method on RMSE of 72-h future predictions are as follows: PM2.5: 6%10%; PM10: 5%7%; NO2: 5%16%; O3: 6%9%. Furthermore, we demonstrate the effectiveness of each module of M2G2 by ablation study. We conduct a sensitivity analysis of air quality and meteorological data, finding that the introduction of O3 adversely impacts the prediction accuracy of PM2.5. Keywords -------- Air quality prediction Multi-spatial scale Multi-temporal scale Graph convolutional network Gate recurrent unit Data availability ----------------- Data will be made available on request. Cited by (0) ------------ © 2024 The Author(s). Published by Elsevier Ltd.""",
        """Rock Mechanics for Natural Resources and Infrastructure Development contains the proceedings of the 14 th ISRM International Congress (ISRM 2019, Foz do Igua莽u, Brazil, 13-19 September 2019). Starting in 1966 in Lisbon, Portugal, the International Society for Rock Mechanics and Rock Engineering (ISRM) holds its Congress every four years. At this 14 th occasion, the Congress brings together researchers, professors, engineers and students around contemporary themes relevant to rock mechanics and rock engineering. Rock Mechanics for Natural Resources and Infrastructure Development contains 7 Keynote Lectures and 449 papers in ten chapters, covering topics ranging from fundamental research in rock mechanics, laboratory and experimental field studies, and petroleum, mining and civil engineering applications. Also included are the prestigious ISRM Award Lectures, the Leopold Muller Award Lecture by professor Peter K. Kaiser. and the Manuel Rocha Award Lecture by Dr. Quinghua Lei. Rock Mechanics for Natural Resources and Infrastructure Development is a must-read for academics, engineers and students involved in rock mechanics and engineering. Proceedings in Earth and geosciences - Volume 6 The roceedings in Earth and geosciences series contains proceedings of peer-reviewed international conferences dealing in earth and geosciences. The main topics covered by the series include: geotechnical engineering, underground construction, mining, rock mechanics, soil mechanics and hydrogeology.""",
        """This volume brings together 18 experts with diverse backgrounds and expertise from around the globe to tackle climate change from multiple angles. A comprehensive exposition of the interconnection between ocean, weather, and climate variability is a pre-requisite for understanding the challenge. The solution approach encompasses a better appreciation of the roof, refined solar energy estimation, heightened heat exchange effectiveness, improved understanding of photovoltaic operation in the Arctic, and integration of thermoelectric with photovoltaic. Adaptation is an essential and immediate remedy that every individual must take part in, understanding that men and women respond to the thermal environment differently. Imagine future buildings made from appetizing materials, closing a sustainable design process with self-sufficient communities. Would hydrogen become a crucial part of the mitigation?""",
        """This book addresses some of the baffling questions encountered at the final frontier of space and time related to particle physics and cosmology in the context of recent iconoclastic observations and developments. When particle physics stagnated in the early seventies, a new development emerged — String Theory.For the past 25 years, String Theory, popularly called the “Theory of Everything”, has mesmerized not just scientists, but also the general public. Yet a closer scrutiny today reveals that it is no more than a mathematical marvel. It has neither predicted anything nor has it been anywhere near verification. We are essentially where we were in the early seventies.Another chronic problem that Einstein had abandoned was the unified description of his theory of General Relativity that deals with the Universe at large and Electromagnetism which is to do with particles. Such a description is necessary even if it warrants a radical departure from our time-honoured ideas of space and time. A bright spot has now appeared in Cosmology. Iconoclastic observations in the past years have shown that the Universe is actually accelerating, driven by a mysterious Dark Energy. This book takes the lay reader through these uncharted waters and mind-boggling developments on an unimaginable journey from the ultra small to the farthest stretches conceivable, via such imagination defying concepts as extra dimensions and multiple universes.""",
        """Nickel Sulfide Ores and Impact Melts: Origin of the Sudbury Igneous Complex presents a current state of understanding on the geology and ore deposits of the Sudbury Igneous Complex in Ontario, Canada. As the first complete reference on the subject, this book explores the linkage between the processes of meteorite impact, melt sheet formation, differentiation, sulfide immiscibility and metal collection, and the localization of ores by magmatic and post-magmatic processes. The discovery of new ore deposits requires industry and government scientists and academic scholars to have access to the latest understanding of ore formation process models that link to the mineralization of their host rocks. The ore deposits at Sudbury are one of the world largest ore systems, representing a classic case study that brings together very diverse datasets and ways of thinking. This book is designed to emphasize concepts that can be applied across a broad range of ore deposit types beyond Sudbury and nickel deposit geology. It is an essential resource for exploration geologists, university researchers, and government scientists, and can be used in rock and mineral analysis, remote sensing, and geophysical applications.Provides the only reference book to focus entirely on the Sudbury Igneous ComplexBrings together an understanding of ore deposit and impact melts as a basis for future explorationAuthored by a leading expert on the geology of the Sudbury Igneous Complex with 35 years of experience working on nickel sulfide ore deposits""",
        """Explore a Viable Resource for DesalinationThe world's freshwater supplies are rapidly depleting and seawater is being positioned as a major feasible replacement in the search for a sustainable water source. Focused on large-scale multi-stage flash (MSF) seawater desalination plants, and based on research conducted on a real 18-stage plant, Multi-St""",
        """The existing theories on the evolution of senescence assume that senescence is inevitable in all organisms. However, recent studies have shown that this is not necessarily true. A better understanding of senescence and its underlying mechanisms could have far-reaching consequences for conservation and eco-evolutionary research. This book is the first to offer interdisciplinary perspectives on the evolution of senescence in many species, setting the stage for further developments. It brings together new insights from a wide range of scientific fields and cutting-edge research done on a multitude of different animals (including humans), plants and microbes, giving the reader a complete overview of recent developments and of the controversies currently surrounding the topic. Written by specialists from a variety of disciplines, this book is a valuable source of information for students and researchers interested in ageing and life history traits and populations.""",
        """Reviews in Mineralogy & Geochemistry (RiMG) volumes contain concise advances in theoretical and/or applied mineralogy, crystallography, petrology, and geochemistry.""",
        """Fluid dynamics is fundamental to our understanding of the atmosphere and oceans. Although many of the same principles of fluid dynamics apply to both the atmosphere and oceans, textbooks tend to concentrate on the atmosphere, the ocean, or the theory of geophysical fluid dynamics (GFD). This textbook provides a comprehensive unified treatment of atmospheric and oceanic fluid dynamics. The book introduces the fundamentals of geophysical fluid dynamics, including rotation and stratification, vorticity and potential vorticity, and scaling and approximations. It discusses baroclinic and barotropic instabilities, wave-mean flow interactions and turbulence, and the general circulation of the atmosphere and ocean. Student problems and exercises are included at the end of each chapter. Atmospheric and Oceanic Fluid Dynamics: Fundamentals and Large-Scale Circulation will be an invaluable graduate textbook on advanced courses in GFD, meteorology, atmospheric science and oceanography, and an excellent review volume for researchers. Additional resources are available at www.cambridge.org/9780521849692.""",
        """This Handbook is an essential reference and a guide to the rapidly expanding field of Geographic Information Science. Designed for students and researchers who want an in-depth treatment of the subject, including background information Comprises around 40 substantial essays, each written by a recognized expert in a particular area Covers the full spectrum of research in GIS Surveys the increasing number of applications of GIS Predicts how GIS is likely to evolve in the near future""",
        """The considerable exploration success achieved by geochemistry over the last several decades - and still continuing - has provided both the basis and rationale for the Handbook of Exploration Geochemistry series, including Volume 6, Drainage Geochemistry in Mineral Exploration. With contributions from 25 experts of truly global professional experience in drainage geochemistry, this book is a thorough appraisal of the state of the art in the use of surface and sub-surface waters, stream and lake sediments, heavy minerals for mineral exploration in tropical rain forests, temperate glaciated terrains, mountain chains, arid deserts and regions of agricultural and industrial pollution. Additional attention is given to gold and uranium exploration, and to the growing role of drainage geochemistry as a multi-purpose environmental mapping technique with applications in human health studies, ore deposit modelling and pollution monitoring. It comprises 16 chapters, more than 250 figures and a bibliography of some 1600 references.This book is the most extensive and detailed single work on the principles and applications of drainage geochemistry in mineral exploration blending both theoretical considerations and practical implementations.""",
        """Most widely available approaches to semantic integration provide ad-hoc, non-systematic, subjective manual mappings that lead to procrustean amalgamations to fit the target standard, an outcome that pleases no one. Written by experts in the field, Theories of Geographic Concepts: Ontological Approaches to Semantic Integration emphasizes the""",

    ]


    for abstract in abstract_list:
        results = milvus_db_manage.search_data(abstract)
        #print(f'results:{results}')
        distance = round(float(results[0][0].distance),4)
        norm_distance = 1 - distance/2
        print(f"Distance: {distance:.4f}, norm distance:{norm_distance:.4f}")

if __name__ == '__main__':
    #start_emb_text_into_milvus(geo_wiki_corpus_file_path, limit_size=2000)
    #start_emb_text_into_milvus(geo_oa_paper_corpus_file_path, limit_size=2000)
    search_text_list()
